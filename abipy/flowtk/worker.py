"""[summary]
"""
import sys
import os
import threading
import json
import socket
import time
import subprocess
import tempfile
import functools
import tornado.web
import pandas as pd
import tornado.escape
import requests
import panel as pn

from datetime import datetime
from pprint import pprint, pformat
from queue import Queue, Empty
from contextlib import closing
from monty import termcolor
from monty.collections import AttrDict #, dict2namedtuple
from monty.string import list_strings #, marquee
from monty.json import MSONable
from monty.functools import lazy_property
from pymatgen.util.serialization import pmg_serialize
from abipy.tools.printing import print_dataframe
from abipy.tools import duck
from abipy.flowtk.flows import Flow
from abipy.flowtk.launcher import MultiFlowScheduler
#from .db import DBConnector


# This should become a global variable used in all the other modules to facilitate unit tests.
_ABIPY_DIR = os.path.join(os.path.expanduser("~"), ".abinit", "abipy")


def yaml_safe_load_path(path):
    import ruamel.yaml as yaml
    with open(path, "rt") as fh:
        return yaml.YAML(typ='safe', pure=True).load(fh.read())


class BaseHandler(tornado.web.RequestHandler):

    def initialize(self, worker):
        self.worker = worker

    def get_json_from_body(self):
        try:
            data = tornado.escape.json_decode(self.request.body)
            return data
        except ValueError:
            # TODO: handle the error
            raise

    #def write_error(self, status_code, **kwargs):
    #    # https://stackoverflow.com/questions/26371051/better-way-to-handle-errors-in-tornado-request-handler
    #   import traceback, logging
    #   if status_code == 500:
    #      excp = kwargs['exc_info'][1]
    #      tb   = kwargs['exc_info'][2]
    #      stack = traceback.extract_tb(tb)
    #      clean_stack = [i for i in stack if i[0][-6:] != 'gen.py' and i[0][-13:] != 'concurrent.py']
    #      error_msg = '{}\n  Exception: {}'.format(''.join(traceback.format_list(clean_stack)),excp)

    #      # do something with this error now... e.g., send it to yourself
    #      # on slack, or log it.
    #      logging.error(error_msg)  # do something with your error...

    #   # don't forget to show a user friendly error page!
    #   self.render("oops.html")



class ActionHandler(BaseHandler):

    def post(self):
        data = self.get_json_from_body()
        action = data.get("action", None)
        if action is None: return
        reply = {}

        if action == "kill":
            reply["message"] = "Received kill action. Aborting now!"
            self.write(reply)
            # TODO: Should lock and give a change to the secondary thread to complete operations
            time.sleep(4)
            sys.exit(0)

        #elif action == "rm_errored_flows":
        #    self.worker.scheduler.rm_flows_with_status("Error")
        #    reply["message"] = f"Removed: {nflows} Errored Flows"

        else:
            reply["message"] = f"Unknown action: {action}"

        self.write(reply)


class PostFlowHandler(BaseHandler):

    def post(self):
        data = self.get_json_from_body()

        pyscript_basename = data["pyscript_basename"]
        pyscript_string = data["pyscript_string"]
        user_message = data["user_message"]

        scratch_dir = self.worker.scratch_dir
        filepath = os.path.join(scratch_dir, pyscript_basename)
        with open(filepath, "wt") as fh:
            fh.write(pyscript_string)

        now = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')
        workdir = tempfile.mkdtemp(prefix=f"Flow-{now}", dir=scratch_dir)

        # FIXME: problem with manager and possible collisions in filepath
        p = subprocess.Popen(f"python {filepath} -w {workdir}",
                             stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)
        out, err = p.communicate()

        reply = dict(
            returncode=p.returncode,
            out=out.decode("utf-8") if out is not None else None,
            err=err.decode("utf-8") if err is not None else None,
            workdir=workdir,
        )

        if p.returncode == 0:
            print(f"Running new Flow generated by {pyscript_basename} in workdir: {workdir}")
            flow = Flow.from_file(workdir)
            self.worker.flow_scheduler.add_flow(flow, user_message)

        self.write(reply)


class _JsonHandler(BaseHandler):

    def prepare(self):
        self.set_header(name="Content-Type", value="application/json")


class JsonStatusHandler(_JsonHandler):

    def get(self):
        # FIXME: This is broken now
        json_status = self.worker.flow_scheduler.get_json_status()
        print("json_status", json_status)
        self.write(json_status)


class FlowDirsPostHandler(BaseHandler):

    def post(self):
        data = self.get_json_from_body()
        user_message = data["user_message"]
        flow_dir = data["flow_dir"]

        error = ""
        try:
            flow = Flow.from_file(flow_dir)
            self.worker.flow_scheduler.add_flow(flow, user_message)
        except Exception as exc:
            error = str(exc)

        reply = dict(
            returncode=0 if not error else 1,
            error=error,
        )

        self.write(reply)


def find_free_port(address="localhost"):

    with closing(socket.socket(socket.AF_INET, socket.SOCK_STREAM)) as s:
        s.bind((address, 0))
        s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        return s.getsockname()[1]


def port_is_open(port, address="localhost"):
    with closing(socket.socket(socket.AF_INET, socket.SOCK_STREAM)) as sock:
        if sock.connect_ex((address, port)) == 0: return True
        return False


def pid_exists(pid):
    """
    Check whether pid exists in the current process table. UNIX only.

    https://stackoverflow.com/questions/568271/how-to-check-if-there-exists-a-process-with-a-given-pid-in-python/6940314#6940314
    """
    # import os
    import errno
    if pid < 0:
        return False
    if pid == 0:
        # According to "man 2 kill" PID 0 refers to every process
        # in the process group of the calling process.
        # On certain systems 0 is a valid PID but we have no way
        # to know that in a portable fashion.
        raise ValueError('invalid PID 0')
    try:
        os.kill(pid, 0)
    except OSError as err:
        if err.errno == errno.ESRCH:
            # ESRCH == No such process
            return False
        elif err.errno == errno.EPERM:
            # EPERM clearly means there's a process to deny access to
            return True
        else:
            # According to "man 2 kill" possible error values are
            # (EINVAL, EPERM, ESRCH)
            raise
    else:
        return True

class WorkerState(AttrDict):
    """

        status is one of the following: "init", "serving", "dead".
    """

    @classmethod
    def new(cls, **kwargs):

        import getpass
        import uuid

        new = cls(
            name=None,
            status="init",
            pid=os.getpid(),
            address="localhost",
            port=0,
            scratch_dir=None,
            username=getpass.getuser(),
            hostname=socket.gethostname(),
            uuid=str(uuid.uuid4()),
        )
        new.update(**kwargs)

        if "uuid" not in new:
            new["uuid"] = str(uuid.uuid4())

        return new

    #@classmethod
    #def from_dict(cls, d):
    #    return cls(**d)

    def __str__(self):
        return self.__class__.__name__ + "\n" + pformat(self, indent=2) + "\n"

    def json_write(self, filepath):
        with open(filepath, "wt") as fp:
            json.dump(self, fp, indent=2)

    @classmethod
    def from_json_file(cls, filepath):
        with open(filepath, "rt") as fp:
            return cls(**json.load(fp))

    @classmethod
    def from_json(cls, json_string):
        return cls(**json.loads(json_string))



class AbipyWorker:

    def __init__(self, name: str, sched_options: dict, scratch_dir: str,
                 address="localhost", port=0):
        """
        Args:
            name: The name of the Worker. Must be unique.
            sched_options:
            scratch_dir:
            address:
                The address the server should listen on for HTTP requests.
            port:
                 Allows specifying a specific port
                 Port 0 means to select an arbitrary unused port
        """
        self.name = name
        self.address, self.port = address, port
        self.pid = os.getpid()

        # url --> callables returning panel objects.
        self.routes = {
            "/": self.serve_homepage,
            r"/flow/\d+": self.serve_panel_flow,
        }

        # url --> tornado handler
        self.extra_patterns = [
            ("/post_flow_script", PostFlowHandler, dict(worker=self)),
            ("/post_flow_dirs", FlowDirsPostHandler, dict(worker=self)),
            ("/json_status", JsonStatusHandler, dict(worker=self)),
            ("/action", ActionHandler, dict(worker=self)),
        ]
        self.config_dir = os.path.join(_ABIPY_DIR, f"worker_{self.name}")
        if not os.path.exists(self.config_dir):
            os.mkdir(self.config_dir)

        self.scratch_dir = scratch_dir
        if not os.path.isdir(scratch_dir):
            raise ValueError(f"Scratch directory: `{scratch_dir}` does not exist!")

        sqldb_path = os.path.join(self.config_dir, "flows.db")
        self.flow_scheduler = MultiFlowScheduler(sqldb_path, **sched_options)

        state_file = os.path.join(self.config_dir, "state.json")
        if os.path.exists(state_file):
            with open(state_file, "rt") as fp:
                d = json.load(fp)
            if d["status"] == "serving":
                pid = d["pid"]
                if pid_exists(pid):
                    raise RuntimeError(f"There's already an AbiPy Worker serving on this machine with pid: {pid}")
                # FIXME This does not work in demon mode!
                print(f"Status 'serving' but pid {pid} does not exist!")

    @classmethod
    def _get_state_path(cls, name):
        config_dir = os.path.join(_ABIPY_DIR, f"worker_{name}")
        state_filepath = os.path.join(config_dir, "state.json")
        if not os.path.exists(state_filepath):
            raise RuntimeError(f"Cannot find state file: `{state_filepath}`")

        with open(state_filepath, "rt") as fp:
            return (json.load(fp), state_filepath)

    @classmethod
    def init_from_config_dir(cls, name):
        d, path = cls._get_state_path(name)

        if d["status"] != "init":
            raise RuntimeError(f"`status` entry in json file `{path}`\n should be `init` while it is `{d['status']}`")

        config_dir = os.path.dirname(path)
        sched_options = yaml_safe_load_path(os.path.join(config_dir, "scheduler.yml"))
        #manager = TaskManager.from_file(os.path.join(config_dir, "manager.yml")
        print("sched_options", sched_options)

        return cls(d["name"], sched_options, d["scratch_dir"],
                   address=d["address"], port=d["port"])

    @classmethod
    def restart_from_config_dir(cls, name, force=False):
        d, path = cls._get_state_path(name)
        pid = d["pid"]

        #if d["status"] != "dead":
        # TODO: status can be set to serving if the worker has been killed by kill -9
        # should implement a restart_hard option and perhaps make a bkp of the database!
        if d["status"] == "serving":
            e_pid = pid_exists(pid)

            msg = f"It seems there's already a worker serving on this machine with pid: {pid}"
            if not force:
                if not e_pid:
                    print(f"Status 'serving' but pid {pid} does not exist!")
                else:
                    raise RuntimeError(msg)
            else:
                print(msg)
                print("but as `force` is specified by the user....")
                if e_pid:
                    raise RuntimeError("But pid exists so it's dangereous")

        # TODO: Problem with the default manager when creating the flow.
        # Replace node_id with uuid4
        #from abipy.flowtk.tasks import TaskManager
        #manager = TaskManager.from_file(os.path.join(config_dir, "manager.yml")
        config_dir = os.path.dirname(path)
        sched_options = yaml_safe_load_path(os.path.join(config_dir, "scheduler.yml"))

        new = cls(d["name"], sched_options, d["scratch_dir"], address=d["address"], port=d["port"])
        print("Remember to execute ldiscover or rdiscover...")
        return new

    def write_state_file(self, status="dead", filepath=None) -> None:
        if filepath is None:
            filepath = os.path.join(self.config_dir, "state.json")

        state = WorkerState.new(
            name=self.name,
            status=status,
            pid=self.pid,
            address=self.address,
            port=self.port,
            scratch_dir=self.scratch_dir,
        )

        state.json_write(filepath)

    #def __str__(self):
    #    lines = []
    #    app = lines.append
    #    app("pid %d" % self.pid)
    #    app(str(self.flow_scheduler))

    #    return "\n".join(lines)

    #def remove_flows(self):
    #    """This requires locking the SQLite database."""

    def serve_homepage(self):

        from socket import gethostname
        import platform
        system, node, release, version, machine, processor = platform.uname()

        header = pn.pane.Markdown(f"""
# Homepage of **{self.name}** {self.__class__.__name__}

Running on {gethostname()} -- system {system} -- Python {platform.python_version()}
""", sizing_mode="stretch_width")

        d = self.flow_scheduler.groupby_status()
        md_lines = ["Refresh the page to update the list of Flow."]
        if d:
            for status, values in d.items():
                if status == "error": status = "Errored"
                md_lines.append(f"## {status.capitalize()} Flows:\n")
                for row in values:
                    workdir, flow_id = row["workdir"], row["flow_id"]
                    md_lines.append(f"- [flow_id: {flow_id}, workdir: {workdir}](/flow/{flow_id})\n")
        else:
            md_lines.append("## Empty Flow list!")

        with open(os.path.join(self.config_dir, "state.json"), "rt") as fp:
            state = json.load(fp)

        from abipy.panels.viewers import JSONViewer
        main = pn.Column(
                header,
                pn.Row("## Server state", JSONViewer(state)),
                #pn.layout.Divider(),
                pn.pane.Markdown("\n".join(md_lines), sizing_mode="stretch_width"),
                sizing_mode="stretch_width",
        )

        from abipy.panels.core import get_abinit_template_cls_kwds
        tmpl_cls, tmpl_kwds = get_abinit_template_cls_kwds()
        return tmpl_cls(main=main, **tmpl_kwds)

    def serve_panel_flow(self):
        # URL example: /flow/123 where 123 is the flow id.
        tokens = pn.state.app_url.split("/")
        #print("In serve_panel_flow with app_url", pn.state.app_url, "tokens:", tokens)
        flow_id = int(tokens[2])
        flow, status = self.flow_scheduler.get_flow_status_by_id(flow_id)
        if flow is not None:
            return flow.get_panel(template=self.panel_template)

        return pn.pane.Alert(f"Cannot find Flow with node ID: {flow_id}", alert_type="danger")

    def serve(self, **serve_kwargs):
        from abipy.panels.core import abipanel #, get_abinit_template_cls_kwds, AbipyParameterized
        self.panel_template = serve_kwargs.pop("panel_template", "FastList")
        abipanel(panel_template=self.panel_template)

        import abipy.panels as mod
        assets_path = os.path.join(os.path.dirname(mod.__file__), "assets")
        serve_kwargs.update(
            show=False,
            static_dirs={"/assets": assets_path},
            websocket_origin="*",
        )

        thread = threading.Thread(target=self.flow_scheduler.start, name="flow_scheduler", daemon=True)
        thread.start()
        #termcolor.enable(False)

        #print("serve_kwargs:", serve_kwargs)
        self.address = serve_kwargs.pop("address", self.address)
        self.port = serve_kwargs.pop("port", self.port)
        if self.port == 0:
            self.port = find_free_port(address=self.address)

        # Now write state.json with the actual port.
        self.write_state_file(status="serving")

        # Register function atexit
        import atexit
        atexit.register(self.write_state_file)

        return pn.serve(self.routes, port=self.port, address=self.address,
                        extra_patterns=self.extra_patterns, **serve_kwargs)


def create_new_worker(worker_name: str, scratch_dir: str):

    config_dir = os.path.join(_ABIPY_DIR, f"worker_{worker_name}")
    errors = []
    eapp = errors.append

    if os.path.exists(config_dir):
        eapp(f"Directory `{config_dir}` already exists!")

    scheduler_path = os.path.join(_ABIPY_DIR, "scheduler.yml")
    manager_path = os.path.join(_ABIPY_DIR, "manager.yml")
    if not os.path.exists(scheduler_path):
        eapp("Cannot find scheduler file at `{scheduler_path}` to initialize the worker!")
    if not os.path.exists(manager_path):
        eapp("Cannot find manager file at `{manager_path}` to initialize the worker!")
    if not os.path.isdir(scratch_dir):
        eapp("scratch_dir at `{scratch_dir}` is not a directory!")

    if errors:
        print("The following problems have been detected:")
        for i, err in enumerate(errors):
            print(f"\t [{i+1}] {err}")
        return 1

    print(f"""
Creating new worker directory `{config_dir}`
Copying manager.yml and scheduler.yml files into it.
You may want to customize these files before running calculations.

After this step, one can use:

    abiw.py start {worker_name}

to start the AbiPy worker. Then use:

    abiw.py ldiscover

to update the list of local clients.
""")
    os.mkdir(config_dir)
    from shutil import copy
    copy(scheduler_path, config_dir)
    copy(manager_path, config_dir)

    worker_state = WorkerState.new(
            name=worker_name,
            scratch_dir=scratch_dir,
    )

    with open(os.path.join(config_dir, "state.json"), "wt") as fp:
        json.dump(worker_state, fp, indent=2)

    return 0


def _get_all_local_worker_states(as_dict=False):
    states, paths = [], []

    worker_dirs = [dirname for dirname in os.listdir(_ABIPY_DIR) if dirname.startswith("worker_")]
    for workdir in worker_dirs:
        state_file = os.path.join(_ABIPY_DIR, workdir, "state.json")
        state = WorkerState.from_json_file(state_file)
        states.append(state)
        paths.append(state_file)

    if as_dict:
        states = {state.name: state for state in states}

    return states


def print_local_workers():
    df = pd.DataFrame(_get_all_local_worker_states())
    print_dataframe(df, title="\nLocal AbiPy Workers:\n")


def discover_local_workers(printout=True):
    worker_states = _get_all_local_worker_states()
    clients = WorkerClients.from_json_file(empty_if_not_file=True)
    clients.update_from_worker_states(worker_states, write_json=True)
    if printout:
        df = clients.get_dataframe()
        df = df[df["is_local_worker"] == True]
        print_dataframe(df, title="Local Workers")
    return clients


def rdiscover(hostnames, printout=True):
    #
    # From https://docs.fabfile.org/en/2.6/api/transfer.html#fabric.transfer.Transfer
    #
    # Most SFTP servers set the remote working directory to the connecting userâ€™s home directory,
    # and (unlike most shells) do not expand tildes (~).
    #
    # For example, instead of saying get("~/tmp/archive.tgz"), say get("tmp/archive.tgz").

    from fabric import Connection
    from io import BytesIO
    print("Make sure the hosts are listed in your ~/.ssh/config file and that ProxyJump is activated if needed")
    worker_states = []
    for host in hostnames:
        print(f"Contacting {host}. It may take some time ...")
        try:
            c = Connection(host)
            result = c.run("ls ~/.abinit/abipy")
            files = result.stdout.split()
            worker_dirs = [f for f in files if f.startswith("worker_")]
            worker_dirs = [os.path.join(".abinit/abipy", d) for d in worker_dirs]
        except Exception as exc:
            print(exc)
            continue

        # Now get the state.json via sfpt.
        for w in worker_dirs:
            state_path = os.path.join(w, "state.json")
            try:
                strio = BytesIO()
                c.get(state_path, local=strio)
                json_bstring = strio.getvalue()
                strio.close()
                worker_states.append(WorkerState.from_json(json_bstring))

            except IOError as exc:
                print(f"Cannot find `state.json` file: {host}@{state_path}. Ignoring error.")
                print(exc)

    clients = WorkerClients.from_json_file(empty_if_not_file=True)
    clients.update_from_worker_states(worker_states, write_json=False)
    for client in clients:
        client.setup_ssh_port_forwarding()

    clients.write_json_file()

    if printout:
        df = clients.get_dataframe()
        df = df[df["is_local_worker"] == False]
        print_dataframe(df, title="Remote Workers")
        print(f"Found {len(worker_states)} clients")
        # Make sure that hostname is listed in your ~/.ssh/config file!
        # Change it accordingly

    return clients


_SSH_CONFIG = None

def _get_ssh_config():
    global _SSH_CONFIG

    if _SSH_CONFIG is None:
        # For the API, see http://docs.paramiko.org/en/stable/api/config.html#config-module-api-documentation
        from paramiko import SSHConfig
        user_config = os.path.expanduser("~/.ssh/config")
        if not os.path.exists(user_config):
            raise RuntimeError(f"Cannot find ssh configuration file at {user_config}")
        _SSH_CONFIG = SSHConfig.from_path(user_config)
        print(_SSH_CONFIG)

    return _SSH_CONFIG

    # Return the set of literal hostnames defined in the SSH config
    # (both explicit hostnames and wildcard entries).
    #ssh_config.get_hostnames()
    #host_conf = ssh_config.lookup(self.hostname)


class ClientError(Exception):
    """Exceptions raised by WorkerClient"""


def need_serving_worker(method):

    @functools.wraps(method)
    def decorated(*args, **kwargs):
        self = args[0]
        if self.worker_state.status != "serving":
            raise ClientError(f"Status of {self.worker_state.name} worker is `{self.worker_state.status}`")

        return method(*args, **kwargs)

    return decorated



class WorkerClient(MSONable):

    def __init__(self, worker_state, is_default_worker=False, timeout=240,
                 is_local_worker=None, ssh_destination=None, local_port=None, saved_uuid=None):

        self.worker_state = WorkerState(**worker_state.copy())

        self.is_default_worker = bool(is_default_worker)
        self.timeout = timeout

        if is_local_worker is None:
            # Well, this is not perfect as one may have a laptop with the same hostname as the remote cluster.
            self.is_local_worker = self.worker_state.hostname == socket.gethostname()
        else:
            self.is_local_worker = bool(is_local_worker)

        if ssh_destination is None:
            if self.is_local_worker:
                self.ssh_destination = ""
            else:
                hostnames = _get_ssh_config().get_hostnames()
                if self.worker_state.hostname not in hostnames:
                    print(f"WARNING: Cannot find `{self.worker_state.hostname}` in `{hostnames}`")

                self.ssh_destination = f"{self.worker_state.username}@{self.worker_state.hostname}"
        else:
            self.ssh_destination = str(ssh_destination)

        if local_port is not None:
            self.local_port = int(local_port)
            self.server_url = f"http://localhost:{self.local_port}"

        else:
            if self.is_local_worker:
                self.local_port = self.worker_state.port
                if self.local_port is None:
                    raise ValueError("If local worker, local_port cannot be None")
                self.server_url = f"http://localhost:{self.local_port}"
            else:
                self.local_port = local_port
                if self.local_port is not None:
                    self.server_url = f"http://localhost:{self.local_port}"
                else:
                    self.server_url = None

        self.saved_uuid = saved_uuid if saved_uuid else self.worker_state.uuid
        #self.db_connector = db_connector

    @pmg_serialize
    def as_dict(self) -> dict:
        return dict(
                worker_state=self.worker_state,
                is_default_worker=self.is_default_worker,
                timeout=self.timeout,
                is_local_worker=self.is_local_worker,
                ssh_destination=self.ssh_destination,
                local_port=self.local_port,
                saved_uuid=self.saved_uuid,
        )

    def setup_ssh_port_forwarding(self):
        info = []
        # Return immediately if port forwarding is not needed.

        if self.is_local_worker or self.worker_state.status != "serving":
            return

        print("in setup_ssh_port_forwarding")

        # Need ssh port-forwarding.
        if self.local_port is None:
            self.local_port = find_free_port()
        else:
            if not port_is_open(self.local_port):
                self.local_port = find_free_port()
            else:
                info.append("port self.local_port is already opened")
                return

        ssh_cmd = f"ssh -N -f -L localhost:{self.local_port}:localhost:{self.worker_state.port} {self.ssh_destination}"
        print(f"executing: `{ssh_cmd}`")
        subprocess.run(ssh_cmd, shell=True)

        #return info

    def update_state(self, worker_state):
        prev_hostname = self.worker_state["hostname"]
        self.worker_state = worker_state.copy()
        # This is needed to preserve the changes done by the user in clients.json
        # if the hostname differs from the one used in the ~/.ssh/config file.
        self.worker_state["hostname"] = prev_hostname

        if self.saved_uuid != worker_state.uuid:
            print("Detected change in worker_state.uuid!")
            self.local_port = None
            if self.is_default_worker:
                self.local_port = worker_state.port

            self.saved_uuid = worker_state.uuid

    def __str__(self):
        return self.to_json()

    def to_json(self) -> str:
        """
        Returns a json string representation of the MSONable object.
        """
        # TODO: monty.to_json should accept **kwargs
        from monty.json import MontyEncoder
        return json.dumps(self, cls=MontyEncoder, indent=2)

    def check_server_url(self):
        if self.server_url is None:
            raise ClientError("server_url cannot be None")
        if self.local_port is None:
            raise ClientError("localport cannot be None")
        if not port_is_open(self.local_port):
            raise ClientError("port {local_port} is not open")

    def set_db_connector(self, db_connector):
        self.db_connector = db_connector

        #with self.db_connector.get_collection() as col:
        # Pipelines?
        #cur = col.find_one(flowspec_status="init")

        #myquery = { "address": { "$gt": "S" } }
        #doc = col.find(myquery)
        #flow_kwargs
        #workdir = ??
        #flow = flow_class(workdir, **flow_kwargs)
        #flow.set_workdir(workdir)
        #self.flow_scheduler.add_flow(flow, user_message="")

    @need_serving_worker
    def send_pyscript(self, filepath, user_message="", end_point="post_flow_script"):
        """
        Send a python script to the server in order to build a new Flow.
        """
        filepath = os.path.expanduser(filepath)
        with open(filepath, "rt") as fp:
            data = dict(
                pyscript_basename=os.path.basename(filepath),
                pyscript_string=fp.read(),
                user_message=user_message,
            )

        url = f"{self.server_url}/{end_point}"
        print(f"Sending `{filepath}` script to {self.worker_state.name} worker at url: {url} ...\n")
        self.check_server_url()

        r = requests.post(url=url, json=data, timeout=self.timeout)

        if r.status_code == 200:
            return r.json()

        raise ClientError(f"\nServer returned status code: {r.status_code}\nwith text:\n{r.text}\n")

    @need_serving_worker
    def send_flow_dir(self, flow_dir, user_message="", end_point="post_flow_dirs"):
        if not self.is_local_worker:
            raise RuntimeError(f"You cannot add a Flow to the remote worker {self.worker_state.name}")

        if not os.path.isdir(flow_dir):
            raise RuntimeError(f"Cannot find flow directory: {flow_dir}")

        url = f"{self.server_url}/{end_point}"
        print(f"Sending `{flow_dir} directory to {self.worker_state.name} worker at url: {url} ...\n")
        self.check_server_url()

        data = dict(flow_dir=flow_dir, user_message=user_message)

        r = requests.post(url=url, json=data, timeout=self.timeout)

        if r.status_code == 200:
            return r.json()

        raise ClientError(f"\nServer returned status code: {r.status_code}\nwith text:\n{r.text}\n")

    @need_serving_worker
    def send_kill_message(self, end_point="action"):
        url = f"{self.server_url}/{end_point}"
        print(f"Sending kill message to {self.worker_state.name} worker at url: {url} ...\n")
        self.check_server_url()

        r = requests.post(url=url, json=dict(action="kill"), timeout=self.timeout)

        if r.status_code == 200:
            return r.json()

        raise ClientError(f"\nServer returned status code: {r.status_code}\nwith text:\n{r.text}\n")

    @need_serving_worker
    def get_json_status(self, end_point="json_status"):
        url = f"{self.server_url}/{end_point}"
        print(f"Sending status request to {self.worker_state.name} worker at url: {url} ...\n")
        self.check_server_url()

        r = requests.get(url=url, timeout=self.timeout)
        #print("ok:", r.ok, "status code:", r.status_code)
        #print(r.text)

        if r.status_code == 200:
            #print(r.text)
            return r.json()
            #json_status = r.json()
            #from pandas.io.json import read_json
            #json_status["dataframe"] = read_json(json_status["dataframe"])
            #print_dataframe(json_status["dataframe"], title="\nWorker Status:\n")
            return json_status

        raise ClientError(f"\nServer returned status code: {r.status_code}\nwith text:\n{r.text}\n")

    @need_serving_worker
    def open_webgui(self):
        url = self.server_url
        print(f"Contacting {self.worker_state.name} worker at url: {url} ...\n")
        self.check_server_url()

        #print(self.worker_state)
        import webbrowser
        webbrowser.open_new_tab(url)


class WorkerClients(list, MSONable):

    @classmethod
    def from_json_file(cls, empty_if_not_file=False, filepath=None):
        if filepath is None:
            filepath = os.path.join(_ABIPY_DIR, "clients.json")
        else:
            filepath = os.path.expanduser(filepath)

        new = cls()
        if empty_if_not_file and not os.path.exists(filepath):
            return new

        if not os.path.exists(filepath):
            raise RuntimeError(f"""
Could not find `{filepath}`
Please execute:

    abiw.py ldiscover

or

    abiw.py rdiscover
to create it""")

        with open(filepath, "rt") as fp:
            d = json.load(fp)
            for w in d["clients"]:
                new.append(WorkerClient.from_dict(w))

        new._validate()
        return new

    #def check_local_port(self):
    #    for client in self:
    #        #print(client.local_port, "\n", client)
    #        if duck.is_intlike(client.local_port):
    #            address = "localhost"
    #            if not port_is_open(client.local_port, address=address):
    #                print(f"WARNING: Nobody is listening to `{address}:{client.local_port}`")
    #                client.local_port = None

    def _validate(self):
        err_lines = []
        app = err_lines.append
        from collections import Counter

        # worker_name must be unique.
        for worker_name, count in Counter((w.worker_state.name for w in self)).items():
            if count > 1:
                app(f"Server name: `{worker_name}` appears: {count} times. This is forbidden!")

        # Only zero or one default server is allowed.
        count = sum(1 if w.is_default_worker == True else 0 for w in self)
        if count not in (0, 1):
            app(f"is_default_worker=True appears `{count}` times. This is forbidden as only one default worker is allowed!")

        #self.check_local_port()

        for client in self:
            #print(client.local_port, "\n", client)
            if duck.is_intlike(client.local_port):
                address = "localhost"
                if not port_is_open(client.local_port, address=address):
                    print(f"WARNING: Nobody is listening to `{address}:{client.local_port}`")
                    client.local_port = None

        if err_lines:
            raise RuntimeError("\n".join(err_lines))

    def __str__(self):
        lines = [str(w) for w in self]
        return (2 * "\n").join(lines)

    def as_dict(self):
        return {"clients": [w.as_dict() for w in self]}

    def write_json_file(self, filepath=None) -> None:
        if filepath is None:
            filepath = os.path.join(_ABIPY_DIR, "clients.json")
        else:
            filepath = os.path.expanduser(filepath)

        # TODO: Compare old with new. return diff
        #old_clients = self.__class__.from_json_file(empty_if_not_file=True)
        #if old_clients is None:

        with open(filepath, "wt") as fp:
           json.dump(self.as_dict(), fp, indent=2)

    def update_from_worker_states(self, worker_states, write_json=True):
        for state in worker_states:
            client = self.select_from_worker_name(state.name, allow_none=True)
            if client is not None:
                client.update_state(state)
            else:
                self.append(WorkerClient(state))

        self._validate()
        if write_json: self.write_json_file()

    #def refresh(self):
    #    old_clients = {client.worker_state.name: client.as_dict() for client in self}

    #    #wnames = set([c.worker_state.name for c in self if c.is_default_worker])
    #    wnames = set(self.get_all_worker_names())
    #    all_lworker_states = _get_all_local_worker_states(as_dict=True)
    #    lworker_states = [state for (name, state) in all_lworker_states.items() if name is wnames]
    #    self.update_from_worker_states(lworker_states)

    #    #remote_hostnames = [client.worker_state.hostname for client in self if not client.is_local_worker]
    #    #print(remote_hostnames)
    #    #rdiscover(remote_hostnames, printout=False)

    #    #for client in self:
    #    #    old_d = client.as_dict()
    #    #    client.refresh()
    #    #    new_d = client.as_dict()
    #    #    print("old_d:\n", pformat(old_d))
    #    #    print("new_d:\n", pformat(new_d))

    #    self._validate()
    #    self.write_json_file()

    def get_dataframe(self):

        #keys = ["is_default_worker", "timeout", "is_local_worker", "ssh_destination",
        #         "local_port", "name",  "status", "pid", "address",  "port scratch_dir",
        #         "username", "hostname" "uuid version"
        #        ]

        rows = []
        for client in self:
            d = client.as_dict()
            d = {k: v for k, v in d.items() if not k.startswith("@")}
            state = d.pop("worker_state")
            d.update(**state)
            rows.append(d)

        df = pd.DataFrame(rows)
        #return df.set_index("name")
        return df

    def print_dataframe(self):
        print_dataframe(self.get_dataframe(), title="\nAbiPy Worker Clients:\n")

    def get_all_worker_names(self):
        return [client.worker_state.name for client in self]

    def select_from_worker_name(self, worker_name, allow_none=False):
        if worker_name is None:
            # If there are n > 1 workers, a default must be set by the user.
            for client in self:
                if client.is_default_worker: return client

            if len(self) == 1: return self[0]

            raise ValueError("Cannot find default worker in list:\n" +
                             f"\t{self.get_all_worker_names()}\n" +
                             "Use:\n\tabiw.py set_default WORKER_NAME")

        for client in self:
           if client.worker_state.name == worker_name: return client

        if allow_none: return None
        raise ValueError(f"Cannot find worker with name `{worker_name}` in list:\n" +
                         f"\t{self.get_all_worker_names()}\n")

    def set_default(self, worker_name):
        the_one = self.select_from_worker_name(worker_name)
        for client in self:
            client.is_default_worker = False
        the_one.is_default_worker = True

        self._validate()
        self.write_json_file()
